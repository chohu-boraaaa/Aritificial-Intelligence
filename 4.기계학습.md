# 기계학습의 의미와 종류

## 기계학습
- 경험을 통해 나중에 유사한 일을 더 효율적으로 처리할 수 있도록 시스템 구조나 파라미터를 바꾸는 것

### 직접 만든 규칙이나 휴리스틱 vs 기계학습 방법
- 직접 만든 규칙이나 휴리스틱 : 복잡, 불충분한 성능
- 기계학습 방법 : 자동으로 분류규칙이나 프로그램 생성

### 연역적 학습
- 연역적 추론을 통한 학습

### 귀납적 학습
- 사례들을 일반화하여 패턴 또는 모델 추출
- 오컴의 면도날 : 가장 간단한 것 선호

## 기계학습의 종류
- 지도학습 : 입력과 출력의 데이터로부터 새로운 입력에 대한 출력 결정
- 비지도학습 : 출력이 없는 데이터로부터 패턴 찾음
- 강화학습 : 출력에 대한 정확한 정보는 주어지지 않지만 평가정보(보상, reward)는 주어지는 문제에 대해 각 상태에서의 행동 결정, 정책 : 각 상태 별로 취할 행동 정해놓은 것  

# 지도학습

## 분류
- 출력이 정해진 부류 중 하나로 결정

### 분류 문제의 학습
- 학습 데이터로 잘 분류할 수 있는 함수 찾는 것
- 함수의 형태는 수학적 함수일 수도 규칙일 수도 있음

### 분류기
- 학습된 함수를 이용하여 데이터를 분류하는 프로그램

#### 이상적인 분류기
- 학습이 사용되지 않은 데이터에 대해서 분류를 잘하는 것
- 일반화 능력이 좋은 것

#### 데이터의 구분
- 학습 데이터 : 분류기를 학습하는데 필요한 데이터, 학습데이터가 많을 수록 유리
- 테스트 데이터 : 학습된 모델의 평가
- 검증 데이터 : 학습 중단 시점을 결정하기 위해 사용하는 데이터

#### 과적합 vs 부적합
- 과적합 : 지나치게 학습, 데이터는 오류나 잡음을 포함하기 때문에 학습되지 않은 데이터에 좋지 않은 성능을 보일 수 있음
- 부적합 : 학습데이터를 충분히 학습하지 않은 상태

#### 과적합 회피 방법
- 검증데이터 오류가 감소하다가 증가하는 시점 학습 중단

#### 분류기의 성능평가
- 정확도 : 옳게 분류한 데이터 개수 / 전체 데이터 개수

##### 데이터 부족한 경우
- K겹 교차검증 : 각 등분을 한 번씩 테스트 데이터로 사용하여 성능평가를 하고 평균값 선택

##### 불균형 부류 데이터 문제
- 특정 부류 데이터가 다른 부류에 지나치게 많은 경우 정확도가 의미 없음

###### 대응방안
- 1. 가중치 고려한 정확도 척도 사용
- 2. 많은 학습데이터를 갖는 부류에서 재표본추출
- 3. 적은 학습데이터를 갖는 부류에 대해서 인공적 데이터 생성

###### SOMTE 알고리즘
- 빈도가 낮은 부류의 학습 데이터를 인공적으로 만들어 내는 방법
- 임의로 낮은 빈도의 학습 데이터 x 선택
- x의 k-근접이웃인 같은 부류의 데이터 선택
- k-근접이웃 중 무작위로 하나 y를 선택
- x와 y를 연결하는 직선상의 무작위 위치에 새로운 데이터 생성

##### 이진분류기의 성능평가
- 이진분류기 : 두개의 부류만을 가지는 분류기

###### 민감도=재현율=진양성률 
- 실제 양성인 것 중에 양성으로 예측

###### 특이도=진음성률
- 실제 음성인 것 중에 음성으로 예측

###### 정밀도=양성예측도
- 양성으로 예측한 것 중 실제 양성

###### 음성 예측도
- 음성으로 예측한 것 중 실제 음성

###### 위양성률
- 실제 음성인데 양성으로 예측 = 1 - 특이도

###### 위발견율
- 양성으로 예측했으나 실제 음성 = 1 - 정밀도

###### 정확도
- 전체 중 정확하게 예측한 것

###### F1측도
- 2(정밀도)*(재현율)/((정밀도)+(재현율))

###### ROC 곡선
- 부류 판정 임계값에 따른 (위양성률, 민감도) 그래프
 
###### AUC
- ROC 곡선에서 곡선 아래의 면적
- 클수록 바람직

## 회귀
- 학습 데이터에 부합되는 출력이 실수인 함수를 찾는 문제

### 성능
- 오차 : 실제값과 예측값의 차이, 테스트 데이터들에 대한 (예측값-실제값)^2의 평균 또는 평균의 제곱근
- 모델의 종류(함수의 종류)에 영향받음

### 경사하강법
- 오차함수 E의 그래디언트 반대방향으로 조금씩 움직여가며 최적의 파라미터를 찾으려는 방법

### 회귀의 과적합 대응방법
- 목적함수 = 오차의합 + 가중치*모델복잡도(벌점항)

### 로지스틱 회귀
- 이진 출력, 이진 분류 문제
- 로지스틱 (시그모이드) 함수이용하여 함수 근사
- 가능도 : 모델이 학습데이터로 생성될 가능성
- 경사하강법 사용하여 학습

### 회귀에서 오차의 편향과 분산 분해
- 오차의 기대값 = 편향^2 + 분산 

### 복잡도가 낮은 단순한 모델을 이용한 회귀의 반복
- 실제 데이터와 회귀 함수의 큰 차이 : 큰 편향 
- 학습된 회귀 함수들 간의 차이 적음 : 작은 분산

### 복잡도가 높은 단순한 모델을 이용한 회귀의 반복
- 실제 데이터와 회귀 함수의 작은 차이 : 작은 편향
- 학습된 회귀 함수들 간의 큰 차이 : 큰 분산

### 편향-분산 트레이드오프
- 편향 : 단순한 모델로 잘못 가정할 때 크게 발생 -> 부적합
- 분산 : 복잡한 모델로 잘못 가정할 때 크게 발생 -> 큰 잡음까지 과적합
- 편향과 분산의 동시 축소 어려움 -> 적합한 복잡도의 모델 선택

## 추천
- 개인별로 맞춤별로 추천
- 사용자에게 맞춤형 정보를 제공하여 정보 검색의 부하를 줄여주는 역할

### 추천데이터

#### 희소 형태
- 비어져 있는 행렬의 부분을 채우는 것이 추천

### 추천 기법

#### 내용 기반 출력
- 고객이 이전에 높게 평가했던 것과 유사한 내용 갖는 대상 추천

#### 협력 필터링

##### 사용자간 협력 필터링
- 추천 대상 사용자와 비슷하 평가를 한 사용자 집합 이용

##### 항목간 협력 필터링
- 항목간 유사도를 구하여 유사 항목 선택

#### 은닉 요소 모델
- 행렬 분해에 기반한 기법

## 결정트리
- 트리형태로 의사결정 지식 표현
- 내부 노드 : 비교 속성
- 간선 : 속성 값
- 단말 노드 : 부류, 대표값

### 노드 분할 과정
- 분할 속성 선택
- 속성 값에 따라 서브 노드 생성
- 데이터를 속성값에 따라 분배

### 결정트리 알고리즘
- 엔트로피 : 섞인 정도가 클수록 큰 값

#### 분할 속성

##### 정보이득 척도
- 클수록 좋은 것
- 단점 : 속성 값이 많으면 지나치게 많은 부분집합으로 분할

##### 정보이득 개선 척도

###### 정보이득비
- 속성 값이 많은 속성에 대해 불이익

###### 지니지수
- 데이터 집합에 대한 지니 값

#### 종류
- ID3 알고리즘 : 범주형 속성 값 데이터에 이용
- C4.5 알고리즘 : 범주형 + 수치형 속성 값 데이터에 이용, ID3 개선
- C5.0 알고리즘 : C4.5 발전된 형태
- CART 알고리즘 : 수치형 속성 값 데이터에 이용

### 회귀를 위한 결정트리
- 출력값이 수치값

#### 분할 속성 선택
- SDR = SD - SD(A)

## 앙상블 학습

### 대중의 지혜
- 무작위로 선택된 많은 사람의 답변을 모은 것이 전문가의 답보다 낫다

### 앙상블 학습
- 일련의 예측모델(분류 또는 회귀모델)을 사용한 모델의 학습

### 붓스트랩
- 주어진 학습 데이터에서 복원추출하여 다수의 학습 데이터 집합을 만들어내는 기법

### 배깅
- 붓스트랩을 통해 여러 개 학습 데이터 집합 생성
- 각 학습 데이터 집합 별로 분류기 또는 회귀모델 생성
- 최종 판정 : 분류기들의 투표나 가중치 투표, 회귀모델들의 평균

#### 랜덤 포레스트
- 분류기로 결정트리를 사용하는 배깅 기법
- Random : 무작위로 선택한 속성 중에서 분할 속성 선택
- Forest : 여러 결정트리로 구성

#### 배깅에 의한 회귀
- 붓스트랩을 통해 다수의 학습데이터 생성
- 학습데이터 별로 회귀모델 생성
- 회귀모델의 평균값으로 최종 회귀모델 생성

### 부스팅
- k개 예측모델을 순차적으로 만들어가는 앙상블 모델
- 오차에 따라 학습 데이터에 가중치 또는 값을 변경해가면서 예측모델 생성

#### AdaBoost
- N개의 학습데이터 d에 대한 초기 가중치 w : 가중치들의 합 1
- 학습 오류값 : 잘못 분류한 학습데이터의 가중치 합으로 표현, 학습 오류값이 0.5미만인 분류기들만 사용

##### 학습
- 학습 오류값이 0.5 미만이 학습되는 경우
- 불류기 신뢰도 알파
- 잘못 판정한 학습 데이터의 가중치 증대
- 제대로 판정한 학습 데이터의 가중치 축소
- 가중치의 합이 1이 되도록 정규화

## 신경망

### 신경세포
- 수상돌기 : 전기화학적 신호를 받아들이는 곳
- 축색돌기 : 특정 임계값 이상이면 신호를 내보내는 곳
- 신경연접 : 수상돌기와 축색돌기 연결부위 - 전달되는 신호의 증폭 또는 감쇄

### 퍼셉트론
- 선형분리가능문제 : OR 연산
- 선형분리불가문제 : XOR 연산

### 다층 퍼셉트론
- 여러 개의 퍼셉트론을 층 구조로 구성한 신경망 모델

#### 다층 퍼셉트론의 학습
- 입력-출력 학습 데이터에 대해서 기대출력과 다층 퍼셉트론의 출력의 차이 즉 오차가 최소가 되도록 가중치 결정하는 것

#### 학습 가능한 다층 퍼셉트론
- 오차 역전파 알고리즘 : 활성화함수를 계단함수에서 시그모이드 함수로 대체, 경사하강법 적용

#### 활성화 함수
- 계단함수 
- 시그모이드함수 : 구간 (0,1) 층 여러개 쌓을 수록 성능 좋아짐, 층을 많이 쌓을 수록 미분한 값이 계속 곱해지는데 층의 크기가 점점 0에 가까워지면 학습이 안되는 한계
- 쌍곡탄젠트함수 : 구간 (-1,1)

#### 다층 퍼셉트론 MLP 동작
입력층, 은닉층, 출력층

#### 다층 퍼셉트론(MLP)의 학습

##### 학습목표
- 기대출력과 MLP 출력이 최대한 비슷해지도록 가중치 변경하는 것 -> 경사하강법 사용

##### 학습 알고리즘
- 오차 역전파 알고리즘
